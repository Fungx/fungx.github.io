---
title: System Design Interview 笔记——1.邻近服务
date: 2025-11-1 21:00:00 +0800
categories: [System Design Interview 笔记]
tags: [system design]     # TAG names should always be lowercase
math: true
---
本章中，我们将设计一个邻近服务(Proximity Service). 邻近服务用于发现用户附近的商家，比如餐馆、酒店等等。
# 第一步 - 理解问题并确定设计范围
> 在收到需求后，可以先大胆向面试官提问，理清楚面试官脑海中的实际需求以及对系统的预期目标。
>
> 面试中的系统设计不可能面面俱到。通过提问，也可以缩小需要设计的范围。
>
> 作为参考，下面记录本章的问答示例，后面的章节不再记录。

|候选人提问|面试官回答|
|--|--|
|用户可以设定搜索半径吗？如果半径内没有足够的商家，系统会扩大搜索吗？|假设只考虑固定半径内的商家。如果时间足够，再讨论没有半径内足够对象的商家。|
|允许的最大半径是多少？可以假设为 20km 吗？|可以。|
|用户可以在 UI 上改变搜索半径吗？|可以。我们有以下选项 0.5km, 1km, 2km, 5km 和 20km|
|商家信息可以被添加、删除、更新吗？这些操作需要被实时反应吗？|商家信息可以被进行这些操作。假设添加、更新的商家信息需要在第二天生效。|
|用户可能在使用时移动，搜索结果会有变化。我们需要刷新用户页面，确保结果是最新的吗？|假设用户移动很慢，不需要实时刷新页面|。

## 功能性需求
- 根据用户的位置与搜索半径，返回所有商家
- 商家可以添加、删除、更新商家信息，但这些信息不需要立刻生效
- 用户可以查看商家的详情
  
## 非功能需求
- 低延迟。用户希望搜索结果能快速返回。
- 数据隐私。位置信息是敏感数据，需要保证数据保存和使用合规。(注：在本章的内容中，实际没什么体现，感觉更需要与现实场景结合。)
- 高可用与扩展性。需要确保我们的系统可以抗住人口密集区域的流量高峰。

## 容量估算
假设我们有 100M 日活跃用户和 200M 商家（注：感觉这个数字有点奇怪，一般商家应该远少于日活用户吧）。

下面我们计算系统的 QPS
- 一天有 $24 \times 60 \times 60= 86,400$ 秒，为了方便计算，取整为 $10^5$ 秒。
- 假设一个用户每天搜索 5 次。
- 搜索 $QPS = \frac{100M \times 5}{10^5}= 5,000$.

# 第二步 - 提出高层设计并得到认同
## API 设计

我们使用 RESTful API 的习惯进行设计。

### GET /v1/search/nearby

这个接口返回了基于特定搜索范围的商家。在实际应用中，搜索结果通常是**分页**的

请求参数:
- latitude, 位置的纬度
- longitude, 位置的经度
- radius, (可选) 搜索半径,默认3米

返回示例:
```
{
    "total": 10,
    "bussinesses": [{business object}]
}
``` 

business object 里包含了渲染搜索结果页需要的所有东西。

> 关于如何设计 RESTfule API 的分页，书中提供了一份[参考文档](https://developer.atlassian.com/server/confluence/pagination-in-the-rest-api).
>
> 核心设计思路是2点：
> 1. 查询接口需要支持传入对页数、数量的限制，可以让调用者有查询“下一页”的能力。
> 例如 http://localhost:8080/confluence/rest/api/space/ds/content/page?limit=5&start=10
> 
> 2. 返回结果需要让调用者知道，是否还有更多内容。例如返回结果中，将`_links`中的`next`链接置空，表示不能再获取下一页了。或者，调用者请求 5 个结果，但只返回了 4 个，也可以让对方感知到后面已经没有数据了。 


### 商家 API

对 bussiness object 的操作 API有以下几个：
- GET /v1/bussines/:id 返回一个商家的详情
- POST /v1/bussines 添加一个商家
- PUT /v1/bussines/:id 更新一个商家的详情
- DELETE /v1/bussines/:id 删除一个商家

## 数据模型
这一节会讨论读/写比例以及模式设计。

### 读/写比例
因为“搜索附近的商家”和“查看商家的详情”这两个功能的使用频率很高，显然，我们的读请求是更多的。
另一方面，因为对商家信息的增删改不是常见的操作，写请求是更少的。

对于一个**读多写少**的系统，类似 MySQL 之类的关系型数据库是很好的工具。

### 数据模式
关键的数据表有 商家表 和 地理空间(geo)索引表。
- 商家表，以 bussiness_id 为主键，表中包含了所有相关的详情信息。
- geo 索引表用于高效地执行空间操作。由于这里涉及 geohash，会在「数据库扩容」一节讨论。

### 高层设计
![架构图](/assets/system-design-interview/high-level-design.png)

#### 负载均衡器
负载均衡器自动将入流量分发到多个服务上。一般，公司内会提供 DNS 来完成路由，并根据 URL 将请求打到不同的服务上。

#### LBS
LBS 根据一个给定的半径和位置，查找附近的商家。
- 读负载高，没有写负载。
- QPS 高。
- 无状态，因而可以轻易地水平扩容。

#### 商家服务
商家服务处理 2 类请求：商家对商家信息的增删改查，QPS 低；用户查询商家详情，QPS 高。

#### 数据库集群
数据库使用主从模式。主库处理写操作，多个从库处理读操作。在这种模式中，主库把数据保存后，复制到从库。由于主从间存在延迟，读取偶尔会出现不一致。但在我们的场景中是可接受的

#### 商家服务与 LBS 的可扩展性
商家服务与 LBS 都是无状态的服务，因此可以简单地在高峰/低谷期水平扩容/缩容。

### 搜索附近商家的算法
#### 二维搜索
最符合直觉的和简单的方式是在当前位置画一个圈，然后搜索圈里的所有商家。
这个过程可以翻译成下面的伪 SQL 查询.

``` SQL
SELECT business_id, latitude, longitude,
FROM bussiness
WHERE (latitude BETWEEN {:my_lat} - radius AND {:my_lat} + radius)
AND (longitude BETWEEN {:my_long} - radius AND {:my_long} + radius)
```

这个查询存在一些问题：搜索效率低，需要全表扫描。即使我们给经度和纬度建立了索引，效率也不会提升太多。问题在于我们需要二维的数据，而每一维返回的数据量依然很大。如下图，通过索引可以快速找到符合经度和纬度的所有数据，而求出他们的交集依然是个效率很低的操作，因为数据量太大了。
![alt text](/assets/system-design-interview//intersect.png)

**问题根因：数据库索引只能提升一个维度的搜索速度。**

由此可以引出另一个思路：能否将二维的数据映射到一维？

下图展示了是 2 类地理空间信息的索引方式。他们的核心思想都是一样的：**把地图划分为更小的区域，构建索引来快速搜索。**
![alt text](/assets/system-design-interview/geo-indexes.png)


#### 方案1 均分格子
一种简单的方法是把世界均分为相等大小的格子，每个格子都包含多个商家，每个商家都属于一个格子。
但是，商家的分布不是均匀的。大城市的商家更多，而大洋某个位置上却不会有商家。这种方式产生了非常不均匀的数据分布。

> 数据不均匀会有什么问题？
> 
> 负载不均：大城市商区的商家密度更高，人流也更多。意味着热点格子需要承担更多的请求，大量的请求都会打到一个分区上，产生热点问题。

#### 方案2 Geohash
Geohash 是一种更好的方案，它把二维的经纬度降为一维的字符与数字串。Geohash 算法通过额外的比特位递归地把世界划分为越来越小的格子。

如下图，先把世界分为4个格子：01,11,00,10. 每一个格子内，又可以再做划分：例如 01 可以划分为：0101，0111，0100，0110。通过比较前缀，就可以知道多个格子是否位于同一个更大的格子内。
![alt text](/assets/system-design-interview/geohash.png)

#### 边界问题
geohash 可以保证相近的格子拥有同样的前缀，但会带来一些边界问题。

1. 2个相邻的位置，可能因为格子划分，而拥有完全不相同的前缀。这会导致下面的查询语句结果漏掉部分邻近的商家
```SQL
   select * from geohash_index where geohash like '9q8zn%'
```
2. 2个相邻的位置，可能有相同的前缀，但被划分到不同的 geohash. 

一个通用的解决办法：查询时不仅指查询当前格子，还要查询周围的格子，来避免遗漏商家。求周围格子的 geohash 可以在固定时间复杂度内实现。

#### 方案3 quadtree
quadtree是一种常见的方案，常用于划分二维空间。它不断地把空间分为4个格子，直到每个格子包含了足够的内容（例如100个商家）。

我们可以在服务启动时，在内存中构建对应的quadtree索引，从而实现快速的查询。
> 目前的场景，商家信息的修改可以等到第二天再生效。也就是每天只需要执行一次quadtree的构建即可满足需求。
> 
> 但假如修改需要实时反映到用户查询上，我们还需要考虑quadtree节点的增删、合并与分裂，似乎会引入较高的复杂度。

![alt text](/assets/system-design-interview/quadtree.png)

#### 方案4 Google S2
S2 是一个常见的专业解决方案，与quadtree类似，他也是一个纯内存方案。他把球体映射到一维索引上。在空间上距离相近的2个点，在映射到一维索引后也是相近的。此外，他还有一些特别的优势：

- S2 很适合实现地理围栏，可以表示任意形状的区域。
  > 某打车软件的地理围栏实现例子：数据库中保存了组成围栏的所有边，给定一个点，向左向右发出一条线，通过[射线法](https://zhuanlan.zhihu.com/p/701467904)判断点是否在围栏内。
- S2 可以指定不同的层级，从而可以实现细粒度的格子大小控制。

#### 方案对比
|特点|geohash|quadtree|
|--|---|---|
|实现难度|低|高，需要自己构建树|
|搜索|支持按具体的半径返回商家|支持返回最近k个商家|
|格子细粒度|格子的精度必须提前确定，才能进行编码。格子大小不能动态变化|格子大小可以根据内容密度动态变化|
|更新|简单。例如删除只需要在数据库删掉geohash与商家id关联的一行|复杂。需要在树上进行删除，引入了遍历、重平衡等机制|

# 第三步 - 深入设计
## 数据库扩容
### 商家表
单个节点或许不能承载所有的商家数据。最简单的解决方式是根据 business_id进行分片。这种方式保证了数据可以均匀地分布在所有分片上，且保持了较低的维护成本。
### 空间索引表
以 geohash 为例，可以有2种存储方式

1. 对于每个 geohash key，在一行内使用 json array 保存所有与它相关的 business_id.
2. 每一行只保存一个 business_id 和 geohash 的映射关系。也就是说一个geohash 可能保存了多行。

方案2是更好的。因为方案1每次修改，都需要先读取所有 business_id。例如插入一个 business_id，需要先扫描整个数组确保没有重复，再把整个数据写回到数据库。此外，对于写入还需要加锁，避免并发时丢失修改。而方案2，任何修改都很简单，不需要额外加锁。

## 空间索引扩容
对于空间索引扩容，分片不一定是个好选择。首先，整个空间索引并不大（只包含了 geohash key 和 business_id），可以轻易地存放在现代的数据库中。但考虑到我们高频率的读取，单台数据库服务器或许不能承载那么高的读请求。

因为邻近商家的功能可以忍受**短暂数据不一致**等问题，我们可以通增加多个只读副本，把读取请求放到副本上，降低单个数据库节点的压力。

## 缓存
引入缓存前，首先要想想 **我们是否真的需要缓存？**

出于以下原因，缓存不是一个必要的选项：
- 我们的负载模式是 读多写少，且整体数据规模较小。请求没有引入太多I/O操作，索引可以放在内存中以最快的速度进行搜索。
- 如果读取性能是瓶颈，我们可以增加数据库副本来提高吞吐量。

## 缓存 key
最直接的选择是缓存用户的坐标以及坐标附近的 business_id, 但这种方式存在问题：1. 手机上报的坐标可能不准确；2.用户可能时刻在移动。 这会导致大量的缓存实际不会有作用。

更合理的方式是让小幅度的坐标移动，依然能够映射到同一个缓存key上。前面提到的 geohash/quadtree 都可以很好的解决这一点。因为一个格子里的所有商家，都可以被映射到相同的key上。

## 缓存的数据类型
我们应该缓存2类数据，缓存可以通过 redis 这类分布式kv数据库实现。

> 考虑到对一致性要求不高，其实这里用本地缓存应该也可以。

1. geohash -> 位于格子里的 business_id 列表
2. business_id -> 商家详情对象

# 追加问题：根据商家类型过滤结果
进一步考虑，如何只返回餐厅类的商家？

由于世界已经通过 geohash 或 quatree 分为了多个小格子，每个格子内的商家数量都是相对少的。因此，我们可以先获取所有商家id，再根据商家的详情去进行过滤。从性能上是可接受的。

# 最终设计
![alt text](/assets/system-design-interview/nearby-final.png)

## 获取邻近商家
1. 客户端发送用户位置以及半径到负载均衡器
2. 负载均衡器把请求转发到 LBS
3. LBS 基于用户位置与半径，计算当前位置所在以及附近的格子
4. LBS 对每个格子，从 "Geohash" 缓存中获取所有商家id
5. LBS 与得到的所有商家id，从 "Business Info" 缓存中查出对应的详情
6. LBS 根据商家详情，再找到符合查询半径的商家，排序并组装结果后，返回给客户端

## 查看、更新、添加或删除商家
所有商家相关的操作 API 都与 LBS 隔离，通过 商家服务(Business Service) 实现。同时，每次对商家数据的操作都需要同步到 redis 缓存中。

# 第四步-总结
本章讲述了邻近服务的实现。我们讨论了多种空间索引：二维搜索、均分格子、Geohash、QuadTree、Google S2.

在深入设计中，我们讨论了如何通过缓存减少延迟，什么应该被缓存以及数据应当怎么缓存。